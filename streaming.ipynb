{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All chat models implement the Runnable interface, which comes with a default implementations of standard runnable methods (i.e. ainvoke, batch, abatch, stream, astream, astream_events).\n",
    "\n",
    "The default streaming implementation provides anIterator (or AsyncIterator for asynchronous streaming) that yields a single value: the final output from the underlying chat model provider.\n",
    "\n",
    "The default implementation does not provide support for token-by-token streaming, but it ensures that the the model can be swapped in for any other model as it supports the same standard interface.\n",
    "\n",
    "The ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o\", base_url=\"https://models.inference.ai.azure.com\")\n",
    "for chunk in chat.stream(\"Write me a 1 verse song about goldfish on the moon\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o\", base_url=\"https://models.inference.ai.azure.com\")\n",
    "async for chunk in chat.astream(\"Write me a 1 verse song about goldfish on the moon\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o\", base_url=\"https://models.inference.ai.azure.com\")\n",
    "idx = 0\n",
    "\n",
    "async for event in chat.astream_events(\n",
    "    \"Write me a 1 verse song about goldfish on the moon\", version=\"v1\"\n",
    "):\n",
    "    idx += 1\n",
    "    if idx >= 5:  # Truncate the output\n",
    "        print(\"...Truncated\")\n",
    "        break\n",
    "    print(event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
